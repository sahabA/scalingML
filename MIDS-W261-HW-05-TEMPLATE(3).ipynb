{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Assignment - HW5 Phase1\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  *Sahab Aslam   \n",
    "__Class:__ MIDS w261 (Section *Your Section Goes Here*, e.g., Fall 2016 Group 1)     \n",
    "__Email:__  *Your UC Berkeley Email Goes Here*@iSchool.Berkeley.edu     \n",
    "__Week:__   5\n",
    "\n",
    "__Due Time:__ 2 Phases. \n",
    "\n",
    "* __HW5 Phase 1__ \n",
    "This can be done on a local machine (with a unit test on the cloud such as AltaScale's PaaS or on AWS) and is due Tuesday, Week 6 by 8AM (West coast time). It will primarily focus on building a unit/systems and for pairwise similarity calculations pipeline (for stripe documents)\n",
    "\n",
    "* __HW5 Phase 2__ \n",
    "This will require the AltaScale cluster and will be due Tuesday, Week 7 by 8AM (West coast time). \n",
    "The focus of  HW5 Phase 2  will be to scale up the unit/systems tests to the Google 5 gram corpus. This will be a group exercise \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents <a name=\"TOC\"></a> \n",
    "\n",
    "1.  [HW Intructions](#1)   \n",
    "2.  [HW References](#2)\n",
    "3.  [HW Problems](#3)   \n",
    "1.  [HW Introduction](#1)   \n",
    "2.  [HW References](#2)\n",
    "3.  [HW  Problems](#3)   \n",
    "    1.0.  [HW5.0](#1.0)   \n",
    "    1.0.  [HW5.1](#1.1)   \n",
    "    1.2.  [HW5.2](#1.2)   \n",
    "    1.3.  [HW5.3](#1.3)    \n",
    "    1.4.  [HW5.4](#1.4)    \n",
    "    1.5.  [HW5.5](#1.5)    \n",
    "    1.5.  [HW5.6](#1.6)    \n",
    "    1.5.  [HW5.7](#1.7)    \n",
    "    1.5.  [HW5.8](#1.8)    \n",
    "    1.5.  [HW5.9](#1.9)    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\">\n",
    "# 1 Instructions\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale\n",
    "DATSCIW261 ASSIGNMENT #5\n",
    "\n",
    "Version 2016-09-25 \n",
    "\n",
    " === INSTRUCTIONS for SUBMISSIONS ===\n",
    "Follow the instructions for submissions carefully.\n",
    "\n",
    "https://docs.google.com/forms/d/1ZOr9RnIe_A06AcZDB6K1mJN4vrLeSmS2PD6Xm3eOiis/viewform?usp=send_form \n",
    "\n",
    "\n",
    "### IMPORTANT\n",
    "\n",
    "HW4 can be completed locally on your computer\n",
    "\n",
    "### Documents:\n",
    "* IPython Notebook, published and viewable online.\n",
    "* PDF export of IPython Notebook.\n",
    "    \n",
    "<a name=\"2\">\n",
    "# 2 Useful References\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "* See async and live lectures for this week\n",
    "\n",
    "<a name=\"3\">\n",
    "# HW Problems\n",
    "[Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.0  <a name=\"1.0\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "- What is a data warehouse? What is a Star schema? When is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Dataware houses are central repositories for current and historical data\n",
    "where all data pools from one or more sources and is used for business intelligence to create \n",
    "analytical reports.\n",
    "\n",
    "Star Schema is a fact table which point to the dimension tables (1 to many). It usually is in 3NF. It is used as a basic implementation of an OLAP cube. It can be used to organize the metadata of a relational database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.1  <a name=\"1.1\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "- In the database world What is 3NF? Does machine learning use data in 3NF? If so why? \n",
    "- In what form does ML consume data?\n",
    "- Why would one use log files that are denormalized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A table is in third normal form if:\n",
    "\n",
    "    A table is in 2nd normal form.\n",
    "    It contains only columns that are non-transitively dependent on the primary key\n",
    "Therefore, in 3NF a primiary key is a composite key.\n",
    "\n",
    "Machine Learning uses 3NF because it saves disk space and it would be time consuming to perform join operations on different tables and then apply the analysis. But ML does not require the data to be in 3NF. Therefore, for performance reasons it is best that log files are denormalized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.2  <a name=\"1.2\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.)\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "- (1) Left joining Table Left with Table Right\n",
    "- (2) Right joining Table Left with Table Right\n",
    "- (3) Inner joining Table Left with Table Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting transactions.dat\n"
     ]
    }
   ],
   "source": [
    "%%writefile transactions.dat\n",
    "Alice Bob|$10|US\n",
    "Sam Sneed|$1|CA\n",
    "Jon Sneed|$20|CA\n",
    "Arnold Wesise|$400|UK\n",
    "Henry Bob|$2|US\n",
    "Yo Yo Ma|$2|CA\n",
    "Jon York|$44|CA\n",
    "Alex Ball|$5|UK\n",
    "Jim Davis|$66|JA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Countries.dat\n"
     ]
    }
   ],
   "source": [
    "%%writefile Countries.dat\n",
    "United States|US\n",
    "Canada|CA\n",
    "United Kingdom|UK\n",
    "Italy|IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lj52.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lj52.py\n",
    "from mrjob.job import MRJob\n",
    " \n",
    "class leftjoin2(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        x = line.split(\"|\")\n",
    "        if len(x) == 3:\n",
    "            yield x[2], (\"lefttable\", x[0], x[1])\n",
    "        else:\n",
    "            yield x[1], (\"righttable\",  x[0] )\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        countries = list()\n",
    "        orders = list()\n",
    "        for val in values:\n",
    "            if val[0]== u'lefttable':\n",
    "                countries.append(val)\n",
    "            else:\n",
    "                orders.append(val)\n",
    "\n",
    "        for c in countries:\n",
    "            if len(orders)==0:\n",
    "                yield None, [key] + c[1:] + [None]\n",
    "            else:\n",
    "                for o in orders:\n",
    "                    yield None, [key] + c[1:] + o[1:]\n",
    "if __name__ == '__main__':\n",
    "    leftjoin2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CA', 'Jon Sneed', '$20', 'Canada']\n",
      "['CA', 'Jon York', '$44', 'Canada']\n",
      "['CA', 'Sam Sneed', '$1', 'Canada']\n",
      "['CA', 'Yo Yo Ma', '$2', 'Canada']\n",
      "['JA', 'Jim Davis', '$66', None]\n",
      "['UK', 'Alex Ball', '$5', 'United Kingdom']\n",
      "['UK', 'Arnold Wesise', '$400', 'United Kingdom']\n",
      "['US', 'Alice Bob', '$10', 'United States']\n",
      "['US', 'Henry Bob', '$2', 'United States']\n",
      "\n",
      "\n",
      "There are 9 records\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from lj52 import leftjoin2\n",
    "mr_job = leftjoin2(args=['Countries.dat','transactions.dat'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        print value\n",
    "        count = count + 1\n",
    "print \"\\n\"\n",
    "print \"There are %s records\" %count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting rj52.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile rj52.py\n",
    "from mrjob.job import MRJob\n",
    " \n",
    "class rightjoin(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        x = line.split(\"|\")\n",
    "        if len(x) == 3:\n",
    "            yield x[2], (\"lefttable\", x[0], x[1])\n",
    "        else:\n",
    "            yield x[1], (\"righttable\",  x[0] )\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        countries = list()\n",
    "        orders = list()\n",
    "        for val in values:\n",
    "            if val[0]== u'lefttable':\n",
    "                countries.append(val)\n",
    "            else:\n",
    "                orders.append(val)\n",
    "\n",
    "        \n",
    "        for o in orders:\n",
    "            if len(countries)==0:\n",
    "                yield None, [key] + o[1:] + [None]\n",
    "            else:\n",
    "                for c in countries:\n",
    "                    yield None, [key] + c[1:] + o[1:]\n",
    "if __name__ == '__main__':\n",
    "    rightjoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CA', 'Jon Sneed', '$20', 'Canada']\n",
      "['CA', 'Jon York', '$44', 'Canada']\n",
      "['CA', 'Sam Sneed', '$1', 'Canada']\n",
      "['CA', 'Yo Yo Ma', '$2', 'Canada']\n",
      "['IT', 'Italy', None]\n",
      "['UK', 'Alex Ball', '$5', 'United Kingdom']\n",
      "['UK', 'Arnold Wesise', '$400', 'United Kingdom']\n",
      "['US', 'Alice Bob', '$10', 'United States']\n",
      "['US', 'Henry Bob', '$2', 'United States']\n",
      "\n",
      "\n",
      "There are 9 records\n"
     ]
    }
   ],
   "source": [
    "from rj52 import rightjoin\n",
    "mr_job = rightjoin(args=['Countries.dat','transactions.dat'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        print value\n",
    "        count = count + 1\n",
    "print \"\\n\"\n",
    "print \"There are %s records\" %count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ij52.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ij52.py\n",
    "from mrjob.job import MRJob\n",
    " \n",
    "class innerjoin(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        x = line.split(\"|\")\n",
    "        if len(x) == 3:\n",
    "            yield x[2], (\"lefttable\", x[0], x[1])\n",
    "        else:\n",
    "            yield x[1], (\"righttable\",  x[0] )\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        countries = list()\n",
    "        orders = list()\n",
    "        for val in values:\n",
    "            if val[0]== u'lefttable':\n",
    "                countries.append(val)\n",
    "            else:\n",
    "                orders.append(val)\n",
    "\n",
    "        for c in countries:\n",
    "            for o in orders:\n",
    "                yield None, [key] + o[1:] + c[1:]\n",
    "if __name__ == '__main__':\n",
    "    innerjoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "['CA', 'Canada', 'Jon Sneed', '$20']\n",
      "['CA', 'Canada', 'Jon York', '$44']\n",
      "['CA', 'Canada', 'Sam Sneed', '$1']\n",
      "['CA', 'Canada', 'Yo Yo Ma', '$2']\n",
      "['UK', 'United Kingdom', 'Alex Ball', '$5']\n",
      "['UK', 'United Kingdom', 'Arnold Wesise', '$400']\n",
      "['US', 'United States', 'Alice Bob', '$10']\n",
      "['US', 'United States', 'Henry Bob', '$2']\n",
      "\n",
      "\n",
      "There are 8 records\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ij52 import innerjoin\n",
    "mr_job = innerjoin(args=['Countries.dat','transactions.dat'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        print value\n",
    "        count = count + 1\n",
    "print \"\\n\"\n",
    "print \"There are %s records\" %count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.3 <a name=\"1.3\"></a> Systems tests on n-grams dataset (Phase1) and full experiment (Phase 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "## 3.  HW5.3.0 Run Systems tests locally (PHASE1)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "A large subset of the Google n-grams dataset\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket/folder on Dropbox and on s3:\n",
    "\n",
    "https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 \n",
    "\n",
    "s3://filtered-5grams/\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "The next cell shows the first 10 lines of the googlebooks-eng-all-5gram-20090715-0-filtered.txt file.\n",
    "\n",
    "\n",
    "__DISCLAIMER__: Each record is already a 5-gram. We should calculate the stripes cooccurrence data from the raw text and not from the 5-gram preprocessed data. Calculatating pairs on this 5-gram is a little corrupt as we will be double counting cooccurences. Having said that this exercise can still pull out some simialr terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: unit/systems first-10-lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-0d227eed86ca>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-0d227eed86ca>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    A BILL FOR ESTABLISHING RELIGIOUS\t59\t59\t54\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%writefile googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n",
    "A BILL FOR ESTABLISHING RELIGIOUS\t59\t59\t54\n",
    "A Biography of General George\t92\t90\t74\n",
    "A Case Study in Government\t102\t102\t78\n",
    "A Case Study of Female\t447\t447\t327\n",
    "A Case Study of Limited\t55\t55\t43\n",
    "A Child's Christmas in Wales\t1099\t1061\t866\n",
    "A Circumstantial Narrative of the\t62\t62\t50\n",
    "A City by the Sea\t62\t60\t49\n",
    "A Collection of Fairy Tales\t123\t117\t80\n",
    "A Collection of Forms of\t116\t103\t82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For _HW 5.4-5.5_,  unit test and regression test your code using the  followings small test datasets:\n",
    "\n",
    "* googlebooks-eng-all-5gram-20090715-0-filtered.txt [see above]\n",
    "* stripe-docs-test [see below]\n",
    "* atlas-boon-test [see below]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2: unit/systems atlas-boon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting atlas-boon-systems-test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile atlas-boon-systems-test.txt\n",
    "atlas boon\t50\t50\t50\n",
    "boon cava dipped\t10\t10\t10\n",
    "atlas dipped\t15\t15\t15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3: unit/systems stripe-docs-test\n",
    "Three terms, A,B,C and their corresponding stripe-docs of co-occurring terms\n",
    "\n",
    "- DocA {X:20, Y:30, Z:5}\n",
    "- DocB {X:100, Y:20}\n",
    "- DocC {M:5, N:20, Z:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"DocA\"\t{\"X\":20, \"Y\":30, \"Z\":5}\r\n",
      "\"DocB\"\t{\"X\":100, \"Y\":20}\r\n",
      "\"DocC\"\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\r\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Stripes for systems test 1 (predefined)\n",
    "############################################\n",
    "\n",
    "with open(\"mini_stripes.txt\", \"w\") as f:\n",
    "    f.writelines([\n",
    "        '\"DocA\"\\t{\"X\":20, \"Y\":30, \"Z\":5}\\n',\n",
    "        '\"DocB\"\\t{\"X\":100, \"Y\":20}\\n',  \n",
    "        '\"DocC\"\\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\\n'\n",
    "    ])\n",
    "!cat mini_stripes.txt   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting buildStripes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile buildStripes.py\n",
    "\n",
    "#import csv, sys\n",
    "from mrjob.job import MRJob\n",
    "#from mrjob.step import MRStep\n",
    "#from mrjob.compat import get_jobconf_value\n",
    "\n",
    "class buildStripes(MRJob):\n",
    "          \n",
    "    #yields words = \"apple blu pink\"\n",
    "    # to apple {'blu': 90} apple {'pink': 90}  blu {'apple': 90}\n",
    "    def mapper(self, _, lines):\n",
    "        words, count, pages, books = lines.split(\"\\t\")\n",
    "        count = int(count)\n",
    "        pages = int(pages)\n",
    "        books = int(books)\n",
    "        allwords = words.split()\n",
    "        for i, word in enumerate(allwords):\n",
    "            other_words = allwords[:]\n",
    "            other_words.pop(i)\n",
    "            x = {}\n",
    "            for each_word in other_words:\n",
    "                y={}\n",
    "                y[each_word] = count\n",
    "                x = {m: x.get(m, 0) + y.get(m,0) for m in set(x) | set(y)}\n",
    "            yield word, x\n",
    "                    \n",
    "    \n",
    "    def reducer(self, keys, values):\n",
    "        stripes = {}\n",
    "        x = {}\n",
    "        for y in values:\n",
    "            x = {m: x.get(m, 0) + y.get(m,0) for m in set(x) | set(y)}\n",
    "        print keys, '\\t', x\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    buildStripes.run()\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atlas \t{'dipped': 15, 'boon': 50}\r\n",
      "boon \t{'atlas': 50, 'dipped': 10, 'cava': 10}\r\n",
      "cava \t{'dipped': 10, 'boon': 10}\r\n",
      "dipped \t{'atlas': 15, 'boon': 10, 'cava': 10}\r\n"
     ]
    }
   ],
   "source": [
    "!cat atlas-boon-systems-test.txt | python buildStripes.py -q > atlas_stripes.txt\n",
    "!cat atlas_stripes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: aws: command not found\n",
      "python: can't open file 'buildStripes.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "# Make Stripes from ngrams for systems test 2\n",
    "###############################################\n",
    "\n",
    "!aws s3 rm --recursive s3://ucb261-hw5/hw5-4-stripes-mj\n",
    "!python buildStripes.py -r emr mj_systems_test.txt \\\n",
    "    --cluster-id=j-2WHMJSLZDGOY5 \\\n",
    "    --output-dir=s3://ucb261-hw5/hw5-4-stripes-mj \\\n",
    "    --file=stopwords.txt \\\n",
    "    --file=mostFrequent/part-00000 \\\n",
    "# Output suppressed    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK: Phase 1\n",
    "Complete 5.4 and 5.5 and systems test them using the above test datasets. Phase 2 will focus on the entire Ngram dataset.\n",
    "\n",
    "To help you through these tasks please verify that your code gives the following results (for stripes, inverted index, and pairwise similarities)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10  Build an cooccureence strips from the atlas-boon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using the atlas-boon systems test\n",
    "atlas boon\t50\t50\t50\n",
    "boon cava dipped\t10\t10\t10\n",
    "atlas dipped\t15\t15\t15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stripe documents for  atlas-boon systems test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "# Make Stripes from ngrams \n",
    "###############################################\n",
    "!aws s3 rm --recursive s3://ucb261-hw5/hw5-4-stripes-mj\n",
    "!python buildStripes.py -r emr mj_systems_test.txt \\\n",
    "    --cluster-id=j-2WHMJSLZDG \\\n",
    "    --output-dir=s3://ucb261-hw5/hw5-4-stripes-mj \\\n",
    "    --file=stopwords.txt \\\n",
    "    --file=mostFrequent/part-00000 \\\n",
    "# Output suppressed    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir stripes-mj\n",
    "!aws s3 sync s3://ucb261-hw5/hw5-4-stripes-mj/  stripes-mj/\n",
    "!cat stripes-mj/part-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"atlas\"\t{\"dipped\": 15, \"boon\": 50}\n",
    "\"boon\"\t{\"atlas\": 50, \"dipped\": 10, \"cava\": 10}\n",
    "\"cava\"\t{\"dipped\": 10, \"boon\": 10}\n",
    "\"dipped\"\t{\"atlas\": 15, \"boon\": 10, \"cava\": 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building stripes execution MR stats: (report times!)\n",
    "    took ~11 minutes on 5 m3.xlarge nodes\n",
    "    Data-local map tasks=188\n",
    "\tLaunched map tasks=190\n",
    "\tLaunched reduce tasks=15\n",
    "\tOther local map tasks=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 20  create inverted index, and calculate pairwise similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting InvertIndex.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile InvertIndex.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONProtocol\n",
    "from collections import Counter\n",
    "\n",
    "class InvertIndex(MRJob):\n",
    "    #MRJob.input_protocol = JSONProtocol\n",
    "    \n",
    "    def mapper(self, key, lines):\n",
    "        key, words = lines.split(\"\\t\")\n",
    "        words = words.strip('{').strip('}')\n",
    "        words= words.split(',')\n",
    "        corpus_length = len(words)\n",
    "        for word in words: \n",
    "           # print \n",
    "            yield word.split(\":\")[0].strip().strip(\"'\"), {key.strip('\"').strip('\"'):corpus_length}\n",
    "            \n",
    "\n",
    "    def reducer(self, keys, values):\n",
    "        stripes = {}\n",
    "        x = {}\n",
    "        for y in values:\n",
    "            x = {m: x.get(m, 0) + y.get(m,0) for m in set(x) | set(y)}\n",
    "        print keys, '\\t', x\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    InvertIndex.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atlas \t{'dipped ': 3, 'boon ': 3}\r\n",
      "boon \t{'atlas ': 2, 'cava ': 2, 'dipped ': 3}\r\n",
      "cava \t{'dipped ': 3, 'boon ': 3}\r\n",
      "dipped \t{'atlas ': 2, 'boon ': 3, 'cava ': 2}\r\n"
     ]
    }
   ],
   "source": [
    "!cat atlas_stripes.txt | python InvertIndex.py -q > atlas_inverted.txt\n",
    "!cat atlas_inverted.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p><strong>Solution 1:</strong> </p>\n",
    "<ol>\n",
    "<li>Create an Inverted Index. </li>\n",
    "<li>Use the output to calculate similarities. </li>\n",
    "<li>Build custom partitioner, re-run the similarity code, and output total order sorted partitions.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Similairity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Similarity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Similarity.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "class Similarity(MRJob):\n",
    "    \n",
    "    doc = list()\n",
    "    \n",
    "    def mapper(self, _, lines):\n",
    "        key, words = lines.split(\"\\t\")\n",
    "        words = words.strip('{').strip('}')\n",
    "        #print words\n",
    "        words= words.split(',')\n",
    "        items = []\n",
    "        for word in words: \n",
    "            word = word.strip().strip(\"'\").strip('\"')\n",
    "            \n",
    "            items.append(word)\n",
    "        if(len(items) >1):\n",
    "            k = [(items[i],items[j] ) for i in range(len(items)) for j in range(i+1, len(items))]\n",
    "            for m in k:\n",
    "                 yield m, 1\n",
    "\n",
    "        \n",
    "    def combiner(self, key, values):\n",
    "        count = 0\n",
    "        for v in values:\n",
    "            count += v\n",
    "        yield key, count\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "        pairs = list()\n",
    "        total = 0\n",
    "        aplusb = 0\n",
    "        aplusb_cosine = 1\n",
    "        overlap = 0\n",
    "        average = 0\n",
    "        for v in values:\n",
    "            total = total + v\n",
    "        #counts = {}\n",
    "        for d in key:\n",
    "            doc, count = d.split(\":\")\n",
    "            count = int(count.strip(\"'\").strip())\n",
    "            aplusb = aplusb + count\n",
    "            aplusb_cosine = aplusb_cosine * (count**.5)\n",
    "            if(overlap == 0 or overlap > count ):\n",
    "                overlap = count\n",
    "                #print overlap\n",
    "            pairs.append(doc)\n",
    "        calc={}\n",
    "        j = total*1.0/(aplusb-total)\n",
    "        c = total*1.0/(aplusb_cosine)\n",
    "        d = 2.0*total/aplusb\n",
    "        o = 1.0*total/overlap\n",
    "        average = (j + c + d + o)/4\n",
    "        calc[\"average\"] = average\n",
    "        calc[\"jaccard\"] = j\n",
    "        calc[\"cosine\"] = c\n",
    "        calc[\"dice\"] = d\n",
    "        calc[\"overlap\"] = o\n",
    "        \n",
    "        \n",
    "        print pairs,\"\\t\", calc\n",
    "        #print \"******\"*5\n",
    "        #print \"done\"\n",
    "        \n",
    "            \n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    Similarity.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!cat mini_stripes.txt | python InvertIndex.py -q > mini_stripes_inverted.txt\n",
    "#!cat mini_stripes_inverted.txt\n",
    "!cat mini_stripes_inverted.txt | python Similarity.py -q --jobconf mapred.reduce.tasks=1 > miniS1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"DocB'\", \"DocA'\"] \t{'average': 0.8207908118985981, 'cosine': 0.8164965809277259, 'dice': 0.8, 'overlap': 1.0, 'jaccard': 0.6666666666666666}\r\n",
      "[\"DocB'\", \"DocC'\"] \t{'average': 0.34672168098165174, 'cosine': 0.35355339059327373, 'dice': 0.3333333333333333, 'overlap': 0.5, 'jaccard': 0.2}\r\n",
      "[\"DocC'\", \"DocA'\"] \t{'average': 0.553861376821216, 'cosine': 0.5773502691896258, 'dice': 0.5714285714285714, 'overlap': 0.6666666666666666, 'jaccard': 0.4}\r\n"
     ]
    }
   ],
   "source": [
    "!cat miniS1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"atlas '\", \"boon '\"] {'average': 0.38956207261596576, 'cosine': 0.40824829046386296, 'dice': 0.4, 'overlap': 0.5, 'jaccard': 0.25}\r\n",
      "[\"atlas '\", \"cava '\"] {'average': 1.0, 'cosine': 0.9999999999999998, 'dice': 1.0, 'overlap': 1.0, 'jaccard': 1.0}\r\n",
      "[\"atlas '\", \"dipped '\"] {'average': 0.38956207261596576, 'cosine': 0.40824829046386296, 'dice': 0.4, 'overlap': 0.5, 'jaccard': 0.25}\r\n",
      "[\"boon '\", \"cava '\"] {'average': 0.38956207261596576, 'cosine': 0.40824829046386296, 'dice': 0.4, 'overlap': 0.5, 'jaccard': 0.25}\r\n",
      "[\"cava '\", \"dipped '\"] {'average': 0.38956207261596576, 'cosine': 0.40824829046386296, 'dice': 0.4, 'overlap': 0.5, 'jaccard': 0.25}\r\n",
      "[\"dipped '\", \"boon '\"] {'average': 0.625, 'cosine': 0.6666666666666667, 'dice': 0.6666666666666666, 'overlap': 0.6666666666666666, 'jaccard': 0.5}\r\n"
     ]
    }
   ],
   "source": [
    "!cat atlas_inverted.txt | python Similarity.py -q --jobconf mapred.reduce.tasks=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.3.1  <a name=\"1.3\"></a> Run systems tests on the CLOUD  (PHASE 1)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Repeat HW5.3.0 on the cloud (AltaScale / AWS/ SoftLayer/ Azure). Make sure all tests give correct results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 2: Full-scale experiment on Google N-gram data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Once you are happy with your test results __ proceed to generating  your results on the Google n-grams dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.3.2  Full-scale experiment: EDA of Google n-grams dataset (PHASE 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "- 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.3.4 OPTIONAL Question: log-log plots (PHASE 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?\n",
    "\n",
    "For more background see:\n",
    "- https://en.wikipedia.org/wiki/Log%E2%80%93log_plot\n",
    "- https://en.wikipedia.org/wiki/Power_law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.4  <a name=\"1.4\"></a> Synonym detection over 2Gig of Data\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "For the remainder of this assignment please feel free to eliminate stop words from your analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">There is also a corpus of stopwords, that is, high-frequency words like \"the\", \"to\" and \"also\" that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. Python's nltk comes with a prebuilt list of stopwords (see below). Using this stopword list filter out these tokens from your analysis and rerun the experiments in 5.5 and disucuss the results of using a stopword list and without using a stopword list.\n",
    "\n",
    "> from nltk.corpus import stopwords\n",
    " stopwords.words('english')\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: A large subset of the Google n-grams dataset as was described above\n",
    "\n",
    "For each HW 5.4 -5.5.1 Please unit test and system test your code with respect \n",
    "to SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations for the \n",
    "SYSTEMS TEST DATASET. Then show the results you get with your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the assignment we will focus on developing methods for detecting synonyms, using the Google 5-grams dataset. At a high level:\n",
    "\n",
    "\n",
    "1. remove stopwords\n",
    "2. get 10,0000 most frequent\n",
    "3. get 1000 (9001-10000) features\n",
    "3. build stripes\n",
    "\n",
    "To accomplish this you must script two main tasks using MRJob:\n",
    "\n",
    "\n",
    "__TASK (1)__ Build stripes for the most frequent 10,000 words using cooccurence information based on\n",
    "the words ranked from 9001,-10,000 as a basis/vocabulary (drop stopword-like terms),\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "\n",
    "__TASK (2)__ Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "#### Design notes for TASK (1)\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for TASK (2).\n",
    "\n",
    "#### Design notes for _TASK (2)_\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. \n",
    "\n",
    "Please report the size of the cluster used and the amount of time it takes to run for the index construction task and for the synonym calculation task. How many pairs need to be processed (HINT: use the posting list length to calculate directly)? Report your  Cluster configuration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.5  <a name=\"1.5\"></a> Evaluation of synonyms that your discovered\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "In this part of the assignment you will evaluate the success of you synonym detector (developed in response to HW5.4).\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined by your measure in HW5.4, and use the synonyms function in the accompanying python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Calculate performance measures:\n",
    "$$Precision (P) = \\frac{TP}{TP + FP} $$  \n",
    "$$Recall (R) = \\frac{TP}{TP + FN} $$  \n",
    "$$F1 = \\frac{2 * ( precision * recall )}{precision + recall}$$\n",
    "\n",
    "\n",
    "We calculate Precision by counting the number of hits and dividing by the number of occurances in our top1000 (opportunities)   \n",
    "We calculate Recall by counting the number of hits, and dividing by the number of synonyms in wordnet (syns)\n",
    "\n",
    "\n",
    "Other diagnostic measures not implemented here:  https://en.wikipedia.org/wiki/F1_score#Diagnostic_Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Performance measures '''\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "#print all the synset element of an element\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()\n",
    "hits = []\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "\n",
    "TOTAL = 0\n",
    "flag = False # so we don't double count, but at the same time don't miss hits\n",
    "\n",
    "## For this part we can use one of three outputs. They are all the same, but were generated differently\n",
    "# 1. the top 1000 from the full sorted dataset -> sortedSims[:1000]\n",
    "# 2. the top 1000 from the partial sort aggragate file -> sims2/top1000sims\n",
    "# 3. the top 1000 from the total order sort file -> head -1000 sims_parts/part-00004\n",
    "\n",
    "top1000sims = []\n",
    "with open(\"sims2/top1000sims\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        top1000sims.append(lisst)\n",
    "    \n",
    "\n",
    "measures = {}\n",
    "not_in_wordnet = []\n",
    "\n",
    "for line in top1000sims:\n",
    "    TOTAL += 1\n",
    "\n",
    "    pair = line[0]\n",
    "    words = pair.split(\" - \")\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in measures:\n",
    "            measures[word] = {\"syns\":0,\"opps\": 0,\"hits\":0}\n",
    "        measures[word][\"opps\"] += 1 \n",
    "    \n",
    "    syns0 = synonyms(words[0])\n",
    "    measures[words[1]][\"syns\"] = len(syns0)\n",
    "    if len(syns0) == 0:\n",
    "        not_in_wordnet.append(words[0])\n",
    "        \n",
    "    if words[1] in syns0:\n",
    "        TP += 1\n",
    "        hits.append(line)\n",
    "        flag = True\n",
    "        measures[words[1]][\"hits\"] += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    syns1 = synonyms(words[1]) \n",
    "    measures[words[0]][\"syns\"] = len(syns1)\n",
    "    if len(syns1) == 0:\n",
    "        not_in_wordnet.append(words[1])\n",
    "\n",
    "    if words[0] in syns1:\n",
    "        if flag == False:\n",
    "            TP += 1\n",
    "            hits.append(line)\n",
    "            measures[words[0]][\"hits\"] += 1\n",
    "            \n",
    "    flag = False    \n",
    "\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for key in measures:\n",
    "    p,r,f = 0,0,0\n",
    "    if measures[key][\"hits\"] > 0 and measures[key][\"syns\"] > 0:\n",
    "        p = measures[key][\"hits\"]/measures[key][\"opps\"]\n",
    "        r = measures[key][\"hits\"]/measures[key][\"syns\"]\n",
    "        f = 2 * (p*r)/(p+r)\n",
    "    \n",
    "    # For calculating measures, only take into account words that have synonyms in wordnet\n",
    "    if measures[key][\"syns\"] > 0:\n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        f1.append(f)\n",
    "\n",
    "    \n",
    "# Take the mean of each measure    \n",
    "print \"—\"*110    \n",
    "print \"Number of Hits:\",TP, \"out of top\",TOTAL\n",
    "print \"Number of words without synonyms:\",len(not_in_wordnet)\n",
    "print \"—\"*110 \n",
    "print \"Precision\\t\", np.mean(precision)\n",
    "print \"Recall\\t\\t\", np.mean(recall)\n",
    "print \"F1\\t\\t\", np.mean(f1)\n",
    "print \"—\"*110  \n",
    "\n",
    "print \"Words without synonyms:\"\n",
    "print \"-\"*100\n",
    "\n",
    "for word in not_in_wordnet:\n",
    "    print synonyms(word),word\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Number of Hits: 31 out of top 1000\n",
    "Number of words without synonyms: 67\n",
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Precision\t0.0280214404967\n",
    "Recall\t\t0.0178598869579\n",
    "F1\t\t0.013965517619\n",
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Words without synonyms:\n",
    "----------------------------------------------------------------------------------------------------\n",
    "[] scotia\n",
    "[] hong\n",
    "[] kong\n",
    "[] angeles\n",
    "[] los\n",
    "[] nor\n",
    "[] themselves\n",
    "[] \n",
    "......."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.6  <a name=\"1.6\"></a> OPTIONAL: using different vocabulary subsets\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "Repeat HW5 using vocabulary words ranked from 8001,-10,000;  7001,-10,000; 6001,-10,000; 5001,-10,000; 3001,-10,000; and 1001,-10,000;\n",
    "Dont forget to report you Cluster configuration.\n",
    "\n",
    "Generate the following graphs:\n",
    "-- vocabulary size (X-Axis) versus CPU time for indexing\n",
    "-- vocabulary size (X-Axis) versus number of pairs processed\n",
    "-- vocabulary size (X-Axis) versus F1 measure, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.7  <a name=\"1.7\"></a> OPTIONAL: filter stopwords\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "There is also a corpus of stopwords, that is, high-frequency words like \"the\", \"to\" and \"also\" that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. Python's nltk comes with a prebuilt list of stopwords (see below). Using this stopword list filter out these tokens from your analysis and rerun the experiments in 5.5 and disucuss the results of using a stopword list and without using a stopword list.\n",
    "\n",
    "> from nltk.corpus import stopwords\n",
    ">> stopwords.words('english')\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.8 <a name=\"1.8\"></a> OPTIONAL \n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "There are many good ways to build our synonym detectors, so for this optional homework, \n",
    "measure co-occurrence by (left/right/all) consecutive words only, \n",
    "or make stripes according to word co-occurrences with the accompanying \n",
    "2-, 3-, or 4-grams (note here that your output will no longer \n",
    "be interpretable as a network) inside of the 5-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.9 <a name=\"1.9\"></a> OPTIONAL \n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Once again, benchmark your top 10,000 associations (as in 5.5), this time for your\n",
    "results from 5.6. Has your detector improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
