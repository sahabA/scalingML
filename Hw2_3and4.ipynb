{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3\n",
    "multinomial naive Bayes model (with no smoothing) for SPAM filteringÂ¶\n",
    "\n",
    "Systems test your code first with the Chinese Example and show the resulting model.\n",
    "\n",
    "Learn a SPAM filtering model from the ENRON dataset provided above. Save the model to file SPAM_Model_MNB.tsv.\n",
    "\n",
    "Show the top 10 terms alphabetically sortig the words increasing and their corresponding model entries.\n",
    "Write a mapreduce job to accomplish this. \n",
    "Show the bottom 10 terms also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ChineseExampleTrain.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ChineseExampleTrain.txt\n",
    "D1\t1\tChinese Beijing Chinese\n",
    "D2\t1\tChinese Chinese Shanghai\n",
    "D3\t1\tChinese Macao\n",
    "D4\t0\tTokyo Japan Chines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ChineseExampleTest.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%writefile ChineseExampleTest.txt\n",
    "D1\t1\tChinese Beijing Chinese\n",
    "D2\t1\tChinese Chinese Shanghai\n",
    "D3\t1\tChinese Macao\n",
    "D4\t0\tTokyo Japan Chinese\n",
    "D5\t0\tChinese Chinese Chinese Tokyo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper322.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper322.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        iD, spam, subject, body = line.split('\\t')\n",
    "        body = body + ' ' + subject\n",
    "    except ValueError:\n",
    "        iD, spam, body = line.split('\\t')\n",
    "    \n",
    "    wordsInThisEmail = defaultdict(int)    \n",
    "    for word in re.findall(r\"[\\w']+\", body):\n",
    "        word = word.lower()\n",
    "        wordsInThisEmail[word] += 1\n",
    "    for word in wordsInThisEmail:\n",
    "        #need id to mark new email -if checking frequncy in spam emails but probably don't need this\n",
    "        #or probably need to count the number of spam and ham emails\n",
    "        print iD , \"\\t\" , spam, \"\\t\", word , \"\\t\" , wordsInThisEmail[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer322.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer322.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "spamWords = defaultdict(int)\n",
    "hamWords = defaultdict(int)\n",
    "spamCount = 0\n",
    "hamCount = 0\n",
    "vocab = set() \n",
    "\n",
    "p_word_spam = defaultdict(int)\n",
    "p_word_ham = defaultdict(int)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    iD, spam, word, count = line.split('\\t')\n",
    "    vocab.add(word)\n",
    "    if(int(spam) == 1):\n",
    "        spamWords[word] += int(count)\n",
    "        spamCount += 1\n",
    "    else:\n",
    "        hamWords[word] += int(count)\n",
    "        hamCount +=1\n",
    "\n",
    "\n",
    "for word in vocab:\n",
    "    #will give 0 for viagara if it was never there in ham\n",
    "    #print \"k\"\n",
    "    #if(spamCount != 0 & hamCount != 0): #just in case, you never know4\n",
    "        #print \"k\"\n",
    "    p_word_spam[word] = (spamWords[word]*1.00/spamCount*1.0)\n",
    "    p_word_ham[word] = hamWords[word]*1.0000/hamCount*1.0\n",
    "\n",
    "    print word, \"\\t\", p_word_ham[word], \"\\t\", p_word_spam[word] #, \"\\t\", p_word_ham[word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper322.py\n",
    "!chmod a+x reducer322.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D4 \t0 \tjapan \t1\r\n",
      "D4 \t0 \tchinese \t1\r\n",
      "D4 \t0 \ttokyo \t1\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"D4\t0\tTokyo Japan Chinese\" | ./mapper321.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob1190655386064476820.jar tmpDir=null\n",
      "16/11/28 07:58:38 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/11/28 07:58:38 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/11/28 07:58:39 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/11/28 07:58:39 WARN hdfs.DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1281)\n",
      "\tat java.lang.Thread.join(Thread.java:1355)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:862)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:600)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:789)\n",
      "16/11/28 07:58:39 WARN hdfs.DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1281)\n",
      "\tat java.lang.Thread.join(Thread.java:1355)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:862)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:600)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:789)\n",
      "16/11/28 07:58:39 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/11/28 07:58:39 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1480281455302_0008\n",
      "16/11/28 07:58:40 INFO impl.YarnClientImpl: Submitted application application_1480281455302_0008\n",
      "16/11/28 07:58:40 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1480281455302_0008/\n",
      "16/11/28 07:58:40 INFO mapreduce.Job: Running job: job_1480281455302_0008\n",
      "16/11/28 07:58:48 INFO mapreduce.Job: Job job_1480281455302_0008 running in uber mode : false\n",
      "16/11/28 07:58:48 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/11/28 07:59:05 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/11/28 07:59:06 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/11/28 07:59:14 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/11/28 07:59:14 INFO mapreduce.Job: Job job_1480281455302_0008 completed successfully\n",
      "16/11/28 07:59:14 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=181\n",
      "\t\tFILE: Number of bytes written=356170\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=361\n",
      "\t\tHDFS: Number of bytes written=186\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=28419\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5931\n",
      "\t\tTotal time spent by all map tasks (ms)=28419\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5931\n",
      "\t\tTotal vcore-seconds taken by all map tasks=28419\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5931\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=29101056\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6073344\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=9\n",
      "\t\tMap output bytes=157\n",
      "\t\tMap output materialized bytes=187\n",
      "\t\tInput split bytes=208\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=187\n",
      "\t\tReduce input records=9\n",
      "\t\tReduce output records=6\n",
      "\t\tSpilled Records=18\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=539\n",
      "\t\tCPU time spent (ms)=1970\n",
      "\t\tPhysical memory (bytes) snapshot=540766208\n",
      "\t\tVirtual memory (bytes) snapshot=4512223232\n",
      "\t\tTotal committed heap usage (bytes)=391979008\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=153\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=186\n",
      "16/11/28 07:59:14 INFO streaming.StreamJob: Output directory: chineseExmodel\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar \\\n",
    "   -mapper /home/cloudera/Downloads/scalingML-master/mapper322.py \\\n",
    "   -reducer /home/cloudera/Downloads/scalingML-master/reducer322.py \\\n",
    "   -input ch.txt\\\n",
    "   -output chineseExmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shanghai  \t0.0 \t0.166666666667\r\n",
      "tokyo  \t0.333333333333 \t0.0\r\n",
      "chinese  \t0.333333333333 \t0.833333333333\r\n",
      "macao  \t0.0 \t0.166666666667\r\n",
      "japan  \t0.333333333333 \t0.0\r\n",
      "beijing  \t0.0 \t0.166666666667\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat chineseExmodel/part-0000*\n",
    "!hdfs dfs -get chineseExmodel/part-00000 cp2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese Beijing Chinese \t  SPAM  \t  SPAM\n",
      "Chinese Chinese Shanghai \t  SPAM  \t  SPAM\n",
      "Chinese Macao \t  SPAM  \t  SPAM\n",
      "Tokyo Japan Chinese \t  HAM  \t  HAM\n",
      "Chinese Chinese Chinese Tokyo \t  HAM  \t  HAM\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "#!hadoop fs -get /home/cloudera/chmodelFinal/part-00000 chModel.txt\n",
    "#!ls\n",
    "import io\n",
    "from collections import defaultdict\n",
    "\n",
    "f = open('cp2.txt', \"r\")\n",
    "\n",
    "## use readlines to read all lines in the file\n",
    "## The variable \"lines\" is a list containing all lines\n",
    "lines = file.read(f).splitlines()\n",
    "#print lines\n",
    "\n",
    "modelSpam = {}\n",
    "modelHam = {} #defaultdict(int)\n",
    "\n",
    "for line in lines:\n",
    "        #print line\n",
    "        word, h, s = line.split('\\t')\n",
    "        word = word.strip(\" \")\n",
    "        modelHam[word] = float(h) *1.0\n",
    "        modelSpam[word] = float(s) *1.0\n",
    "##print modelSpam\n",
    "#print float(modelHam['chinese'])\n",
    "f.close()\n",
    "\n",
    "#get the lines from mapper\n",
    "currentID = 0\n",
    "vocab = set() # all the words in email\n",
    "accurate = 0\n",
    "total = 0\n",
    "f = open('/home/cloudera/Downloads/scalingML-master/ChineseExampleTest.txt', \"r\")\n",
    "\n",
    "\n",
    "## use readlines to read all lines in the file\n",
    "## The variable \"lines\" is a list containing all lines\n",
    "lines = file.read(f).splitlines()\n",
    "#a=\"shanghai shangha 6 7\"\n",
    "\n",
    "\n",
    "#print a.split('\\t')\n",
    "for line in lines:\n",
    "    \n",
    "    #print line\n",
    "    iD, spam, words = line.split('\\t')\n",
    "    p_h = 1.0\n",
    "    p_s = 1.0\n",
    "    for word in words.split():\n",
    "        word = word.lower()\n",
    "        p_h  = p_h*float(modelHam[word])\n",
    "        p_s =  p_s*float(modelSpam[word])\n",
    "\n",
    "    if(int(spam)== 1):\n",
    "        c = \"SPAM\"\n",
    "    else:\n",
    "        c = \"HAM\"\n",
    "\n",
    "    if p_s > p_h:\n",
    "        p_c = \"SPAM\"\n",
    "    else:\n",
    "        p_c = \"HAM\"\n",
    "\n",
    "    print words, \"\\t \", p_c, \" \\t \", c\n",
    "\n",
    "    if(p_c == c):\n",
    "\n",
    "        accurate += 1\n",
    "    total +=1\n",
    "\n",
    "    # vocab.add(word)\n",
    "    #currentID = iD\n",
    "        \n",
    "print \"Accuracy: \", (accurate*1.0/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob1924883266190749431.jar tmpDir=null\n",
      "16/11/28 08:07:18 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/11/28 08:07:19 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/11/28 08:07:19 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/11/28 08:07:19 WARN hdfs.DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1281)\n",
      "\tat java.lang.Thread.join(Thread.java:1355)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:862)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:600)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:789)\n",
      "16/11/28 08:07:19 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/11/28 08:07:19 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1480281455302_0010\n",
      "16/11/28 08:07:20 INFO impl.YarnClientImpl: Submitted application application_1480281455302_0010\n",
      "16/11/28 08:07:20 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1480281455302_0010/\n",
      "16/11/28 08:07:20 INFO mapreduce.Job: Running job: job_1480281455302_0010\n",
      "16/11/28 08:07:28 INFO mapreduce.Job: Job job_1480281455302_0010 running in uber mode : false\n",
      "16/11/28 08:07:28 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/11/28 08:07:40 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/11/28 08:07:41 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/11/28 08:07:49 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/11/28 08:07:49 INFO mapreduce.Job: Job job_1480281455302_0010 completed successfully\n",
      "16/11/28 08:07:49 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=603111\n",
      "\t\tFILE: Number of bytes written=1562516\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216481\n",
      "\t\tHDFS: Number of bytes written=192039\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18918\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6233\n",
      "\t\tTotal time spent by all map tasks (ms)=18918\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6233\n",
      "\t\tTotal vcore-seconds taken by all map tasks=18918\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=6233\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=19372032\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6382592\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=15594\n",
      "\t\tMap output bytes=571917\n",
      "\t\tMap output materialized bytes=603117\n",
      "\t\tInput split bytes=214\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=603117\n",
      "\t\tReduce input records=15594\n",
      "\t\tReduce output records=5490\n",
      "\t\tSpilled Records=31188\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=354\n",
      "\t\tCPU time spent (ms)=3230\n",
      "\t\tPhysical memory (bytes) snapshot=539475968\n",
      "\t\tVirtual memory (bytes) snapshot=4512399360\n",
      "\t\tTotal committed heap usage (bytes)=391979008\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216267\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=192039\n",
      "16/11/28 08:07:49 INFO streaming.StreamJob: Output directory: nbmodelNoSmooth\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar \\\n",
    "   -D mapreduce.partition.keycomparator.options=\"-k1,1\" \\\n",
    "   -mapper /home/cloudera/Downloads/scalingML-master/mapper322.py \\\n",
    "   -reducer /home/cloudera/Downloads/scalingML-master/reducer322.py \\\n",
    "   -input NB/e3.txt\\\n",
    "   -output nbmodelNoSmooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottom 10\n",
      "considered  \t0.0 \t0.000120627261761\n",
      "screaming  \t0.0 \t0.000120627261761\n",
      "sending  \t0.0 \t0.000120627261761\n",
      "audio  \t0.0 \t0.000241254523522\n",
      "linda's  \t0.00013691128149 \t0.0\n",
      "wood  \t0.00013691128149 \t0.0\n",
      "advice  \t0.00013691128149 \t0.000361881785283\n",
      "writing  \t0.00013691128149 \t0.000723763570567\n",
      "scotty  \t0.00013691128149 \t0.0\n",
      "reclaimers  \t0.0 \t0.000120627261761\n",
      "Top 10\n",
      "my  \t0.0052026286966 \t0.00772014475271\n",
      "shall  \t0.00013691128149 \t0.000965018094089\n",
      "percent  \t0.0 \t0.000120627261761\n",
      "scenario  \t0.00013691128149 \t0.0\n",
      "limitless  \t0.0 \t0.000241254523522\n",
      "click  \t0.00013691128149 \t0.00470446320869\n",
      "abidjan  \t0.0 \t0.000241254523522\n",
      "romeo  \t0.00013691128149 \t0.0\n",
      "utilities'transmission  \t0.000273822562979 \t0.0\n",
      "entirely  \t0.0 \t0.000241254523522\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -get nbmodelNoSmooth/part-00000 /home/cloudera/Downloads/scalingML-master/SPAM_Model_MNB3.tsv\n",
    "print \"Bottom 10\"\n",
    "!head SPAM_Model_MNB3.tsv\n",
    "print \"Top 10\"\n",
    "!tail SPAM_Model_MNB3.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.4\n",
    "\n",
    "HW 2.3.4 Classify Documents using the learnt Multinomial Naive Bayes model using Hadoop Streaming\n",
    "\n",
    "Classify each Enron email messages using the learnt Naive Bayes classifier (Testing on the training set is bad practice but we will allow that here to simplify the work here).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer HAM SPAM 1.92422639962e-18 0.000844390832328\n",
      "0001.1999-12-10.kaminski HAM HAM 4.04068301655e-12 0.0\n",
      "0001.2000-01-17.beck HAM HAM 1.02162401469e-115 0.0\n",
      "0001.2000-06-06.lokay HAM SPAM 1.77014802049e-132 0.0182147165259\n",
      "0001.2001-02-07.kitchen HAM SPAM 8.46723575782e-133 2.10628751941e-11\n",
      "0001.2001-04-02.williams HAM HAM 7.80699305596e-212 0.0\n",
      "0002.1999-12-13.farmer HAM SPAM 2.54625763151e-162 3.05569661879e-07\n",
      "0002.2001-02-07.kitchen HAM HAM 1.41538955021e-175 0.0\n",
      "0002.2001-05-25.SA_and_HP SPAM SPAM 0.0 2.34010218775e-233\n",
      "0002.2003-12-18.GP SPAM HAM 1.35433981219e-12 1.65363521005e-176\n",
      "0002.2004-08-01.BG SPAM SPAM 0.0 1.44049779701e-319\n",
      "0003.1999-12-10.kaminski HAM HAM 6.42997382469e-161 0.0\n",
      "0003.1999-12-14.farmer HAM HAM 1.42335387812e-31 0.0\n",
      "0003.2000-01-17.beck HAM HAM 2.14404458219e-166 0.0\n",
      "0003.2001-02-08.kitchen HAM SPAM 6.69862024157e-196 2.10628751941e-11\n",
      "0003.2003-12-18.GP SPAM SPAM 0.0 0.000120627261761\n",
      "0003.2004-08-01.BG SPAM SPAM 0.0 3.96434843705e-304\n",
      "0004.1999-12-10.kaminski HAM HAM 6.03319381064e-102 0.0\n",
      "0004.1999-12-14.farmer HAM SPAM 2.3123192494e-102 5.33558961494e-12\n",
      "0004.2001-04-02.williams HAM SPAM 7.45060435042e-268 3.25857040558e-17\n",
      "0004.2001-06-12.SA_and_HP SPAM SPAM 0.0 1.24103791352e-16\n",
      "0004.2004-08-01.BG SPAM HAM 2.4736007336e-13 8.74798235355e-312\n",
      "0005.1999-12-12.kaminski HAM HAM 3.41537290704e-309 0.0\n",
      "0005.1999-12-14.farmer HAM HAM 5.69427776416e-98 0.0\n",
      "0005.2000-06-06.lokay HAM HAM 9.27272665086e-168 0.0\n",
      "0005.2001-02-08.kitchen HAM HAM 5.45183807668e-27 0.0\n",
      "0005.2001-06-23.SA_and_HP SPAM SPAM 0.0 7.10541095474e-74\n",
      "0005.2003-12-18.GP SPAM SPAM 0.0 7.27528821356e-40\n",
      "0006.1999-12-13.kaminski HAM HAM 5.1701018828e-193 0.0\n",
      "0006.2001-02-08.kitchen HAM SPAM 6.76019280871e-222 2.50999262731e-09\n",
      "0006.2001-04-03.williams HAM HAM 3.75560098469e-118 0.0\n",
      "0006.2001-06-25.SA_and_HP SPAM SPAM 0.0 3.85690942956e-150\n",
      "0006.2003-12-18.GP SPAM SPAM 0.0 1.44569087688e-122\n",
      "0006.2004-08-01.BG SPAM HAM 7.12298561968e-07 3.72838631359e-88\n",
      "0007.1999-12-13.kaminski HAM SPAM 2.86626772752e-265 0.000120627261761\n",
      "0007.1999-12-14.farmer HAM SPAM 4.11094036078e-270 1.92409364899e-08\n",
      "0007.2000-01-17.beck HAM HAM 9.4354611645e-101 0.0\n",
      "0007.2001-02-09.kitchen HAM SPAM 1.28457067919e-322 1.93076355946e-09\n",
      "0007.2003-12-18.GP SPAM HAM 0.00177984665936 2.42931114534e-169\n",
      "0007.2004-08-01.BG SPAM HAM 1.68702290993e-07 1.39770245863e-251\n",
      "0008.2001-02-09.kitchen HAM HAM 2.1744615665e-27 0.0\n",
      "0008.2001-06-12.SA_and_HP SPAM SPAM 0.0 1.24103791352e-16\n",
      "0008.2001-06-25.SA_and_HP SPAM SPAM 0.0 4.68811319815e-77\n",
      "0008.2003-12-18.GP SPAM SPAM 0.0 1.24469041624e-65\n",
      "0008.2004-08-01.BG SPAM SPAM 0.0 2.3281498048e-07\n",
      "0009.1999-12-13.kaminski HAM SPAM 1.75360734957e-172 0.000120627261761\n",
      "0009.1999-12-14.farmer HAM SPAM 4.72551312174e-234 0.000241254523522\n",
      "0009.2000-06-07.lokay HAM HAM 7.47548303494e-82 0.0\n",
      "0009.2001-02-09.kitchen HAM HAM 4.08911753561e-267 0.0\n",
      "0009.2001-06-26.SA_and_HP SPAM HAM 0.000133049873496 1.13211541795e-167\n",
      "0009.2003-12-18.GP SPAM SPAM 0.0 6.09056357779e-305\n",
      "0010.1999-12-14.farmer HAM HAM 5.4106128363e-215 0.0\n",
      "0010.1999-12-14.kaminski HAM HAM 4.02731371648e-79 0.0\n",
      "0010.2001-02-09.kitchen HAM HAM 2.62403181037e-208 0.0\n",
      "0010.2001-06-28.SA_and_HP SPAM HAM 2.36105190021e-10 2.51973479379e-322\n",
      "0010.2003-12-18.GP SPAM SPAM 0.0 2.58792748342e-22\n",
      "0010.2004-08-01.BG SPAM SPAM 0.0 6.73731497124e-271\n",
      "0011.1999-12-14.farmer HAM HAM 5.34507587195e-96 0.0\n",
      "0011.2001-06-28.SA_and_HP SPAM HAM 2.36105190021e-10 2.07957170991e-319\n",
      "0011.2001-06-29.SA_and_HP SPAM HAM 0.00013691128149 1.70987390329e-223\n",
      "0011.2003-12-18.GP SPAM SPAM 0.0 6.16579439447e-205\n",
      "0011.2004-08-01.BG SPAM HAM 3.90619854483e-10 4.80765811934e-261\n",
      "0012.1999-12-14.farmer HAM HAM 5.24433129916e-272 0.0\n",
      "0012.1999-12-14.kaminski HAM HAM 8.51424770868e-57 0.0\n",
      "0012.2000-01-17.beck HAM HAM 4.19472422796e-94 0.0\n",
      "0012.2000-06-08.lokay HAM SPAM 2.23061769686e-33 8.48890578948e-22\n",
      "0012.2001-02-09.kitchen HAM SPAM 1.54697633753e-192 0.000241254523522\n",
      "0012.2003-12-19.GP SPAM HAM 0.000273822562979 2.52836785862e-61\n",
      "0013.1999-12-14.farmer HAM HAM 8.16976328942e-43 0.0\n",
      "0013.1999-12-14.kaminski HAM HAM 2.76147286644e-154 0.0\n",
      "0013.2001-04-03.williams HAM HAM 6.81483252309e-261 0.0\n",
      "0013.2001-06-30.SA_and_HP SPAM HAM 0.00355969331873 4.05663706919e-81\n",
      "0013.2004-08-01.BG SPAM SPAM 0.0 8.3045224622e-260\n",
      "0014.1999-12-14.kaminski HAM HAM 1.60213370411e-29 0.0\n",
      "0014.1999-12-15.farmer HAM HAM 6.03502895335e-139 0.0\n",
      "0014.2001-02-12.kitchen HAM HAM 1.86606471613e-246 0.0\n",
      "0014.2001-07-04.SA_and_HP SPAM HAM 1.06287531522e-11 4.03203297378e-82\n",
      "0014.2003-12-19.GP SPAM SPAM 0.0 6.277734552e-68\n",
      "0014.2004-08-01.BG SPAM HAM 0.00123220153341 1.64281767899e-319\n",
      "0015.1999-12-14.kaminski HAM HAM 2.60877895308e-266 0.0\n",
      "0015.2000-06-09.lokay HAM SPAM 6.04009833746e-56 1.15604441657e-12\n",
      "0015.2001-02-12.kitchen HAM SPAM 7.65270226961e-11 1.54461084757e-09\n",
      "0015.2001-07-05.SA_and_HP SPAM SPAM 0.0 2.31776657163e-30\n",
      "0015.2003-12-19.GP SPAM SPAM 0.0 5.28081586708e-175\n",
      "0016.1999-12-15.farmer HAM HAM 1.17306994085e-290 0.0\n",
      "0016.2001-02-12.kitchen HAM HAM 9.92319096101e-141 0.0\n",
      "0016.2001-07-05.SA_and_HP SPAM SPAM 0.0 2.31776657163e-30\n",
      "0016.2001-07-06.SA_and_HP SPAM HAM 0.00355969331873 1.77155811e-223\n",
      "0016.2003-12-19.GP SPAM SPAM 0.0 3.18135291129e-313\n",
      "0016.2004-08-01.BG SPAM SPAM 0.0 1.20369542134e-252\n",
      "0017.1999-12-14.kaminski HAM SPAM 3.11531277254e-149 2.44455729504e-05\n",
      "0017.2000-01-17.beck HAM HAM 4.19472422796e-94 0.0\n",
      "0017.2001-04-03.williams HAM HAM 1.31953616736e-172 0.0\n",
      "0017.2003-12-18.GP SPAM SPAM 0.0 1.99714123135e-81\n",
      "0017.2004-08-01.BG SPAM SPAM 0.0 1.5103249732e-41\n",
      "0017.2004-08-02.BG SPAM HAM 0.000273822562979 5.08830363216e-41\n",
      "0018.1999-12-14.kaminski HAM SPAM 1.41957011362e-59 8.62355591911e-14\n",
      "0018.2001-07-13.SA_and_HP SPAM HAM 7.09958040951e-08 9.61147388222e-287\n",
      "0018.2003-12-18.GP SPAM SPAM 0.0 1.47849698437e-30\n",
      "Accuracy:  0.636363636364\n",
      "Zeros Ham:  28  Spam:  37\n",
      "Accuracy for non-zero  1.85294117647\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import re\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "f = open('/home/cloudera/Downloads/scalingML-master/SPAM_Model_MNB3.tsv', \"r\")\n",
    "\n",
    "## use readlines to read all lines in the file\n",
    "## The variable \"lines\" is a list containing all lines\n",
    "lines = file.read(f).splitlines()\n",
    "#print lines\n",
    "\n",
    "modelSpam = {}\n",
    "modelHam = {} #defaultdict(int)\n",
    "\n",
    "for line in lines:\n",
    "        #print line\n",
    "        word, h, s = line.split('\\t')\n",
    "        word = word.strip(\" \")\n",
    "        modelHam[word] = float(h) *1.0\n",
    "        modelSpam[word] = float(s) *1.0\n",
    "f.close()\n",
    "\n",
    "#get the lines from mapper\n",
    "currentID = 0\n",
    "vocab = set() # all the words in email\n",
    "accurate = 0\n",
    "total = 0\n",
    "f = open('/home/cloudera/NaiveBayes/enronemail_1h.txt', \"r\")\n",
    "\n",
    "\n",
    "## use readlines to read all lines in the file\n",
    "## The variable \"lines\" is a list containing all lines\n",
    "lines = file.read(f).splitlines()\n",
    "#a=\"shanghai shangha 6 7\"\n",
    "p_h_z = 0 #number of zero probs\n",
    "p_s_z = 0\n",
    "\n",
    "#print a.split('\\t')\n",
    "for line in lines:\n",
    "    try:\n",
    "    #print line\n",
    "        iD, spam, subject, body = line.split('\\t')\n",
    "        words = body + ' ' + subject\n",
    "    except ValueError:\n",
    "        iD, spam, words = line.split('\\t')\n",
    "    #iD, spam, words = line.split('\\t')\n",
    "    p_h = 1.0\n",
    "    p_s = 1.0\n",
    "    for word in re.findall(r\"[\\w']+\", words):\n",
    "        #words.split():\n",
    "        word = word.lower()\n",
    "        #print \"words\" , word, float(modelHam[word])\n",
    "        if modelHam[word]>0.0 and p_h > 0.0:\n",
    "            if p_h is not 1.0:\n",
    "                #print p_h\n",
    "                p_h  = math.log(p_h)+math.log(float(modelHam[word]))\n",
    "               # print p_h\n",
    "                p_h = math.exp(p_h)\n",
    "                #print p_h\n",
    "            else: #\n",
    "                p_h = float(modelHam[word])\n",
    "        else:\n",
    "            p_h = float(modelHam[word])\n",
    "        \n",
    "        \n",
    "        if modelSpam[word]>0.0 and p_s > 0.0:\n",
    "            if p_s is not 1.0:\n",
    "                p_s  = math.log(p_s)+math.log(float(modelSpam[word]))\n",
    "                p_s = math.exp(p_s)\n",
    "                #print p_s\n",
    "            else:\n",
    "                p_s = float(modelSpam[word])\n",
    "        else:\n",
    "            p_s = float(modelSpam[word])\n",
    "        \n",
    "    if(int(spam)== 1):\n",
    "        c = \"SPAM\"\n",
    "    else:\n",
    "        c = \"HAM\"\n",
    "        \n",
    "    if p_s == 0.0 :\n",
    "        p_s_z +=1\n",
    "    if p_h == 0.0 :\n",
    "        p_h_z +=1\n",
    "    if(p_s != 0.0 or p_h != 0.0):\n",
    "        if p_s > p_h:\n",
    "            p_c = \"SPAM\"\n",
    "        else:\n",
    "            p_c = \"HAM\"\n",
    "        print iD, c, p_c, p_h, p_s\n",
    "\n",
    "        if(p_c == c):\n",
    "\n",
    "            accurate += 1\n",
    "        total +=1\n",
    "\n",
    "    # vocab.add(word)\n",
    "    #currentID = iD\n",
    "        \n",
    "print \"Accuracy: \", (accurate*1.0/total)\n",
    "print \"Zeros Ham: \", p_h_z, \" Spam: \",p_s_z\n",
    "print \"Accuracy for non-zero \" , (accurate)*1.0/(total - (p_h_z+p_s_z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer341.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer341.py\n",
    "#!/usr/bin/env python\n",
    "#Laplace smoothing\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "spamWords = defaultdict(int)\n",
    "hamWords = defaultdict(int)\n",
    "spamCount = 0\n",
    "hamCount = 0\n",
    "vocab = set() \n",
    "\n",
    "p_word_spam = defaultdict(int)\n",
    "p_word_ham = defaultdict(int)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    iD, spam, word, count = line.split('\\t')\n",
    "    vocab.add(word)\n",
    "    if(int(spam) == 1):\n",
    "        spamWords[word] += int(count)\n",
    "        spamCount += 1\n",
    "    else:\n",
    "        hamWords[word] += int(count)\n",
    "        hamCount +=1\n",
    "\n",
    "\n",
    "for word in vocab:\n",
    "    p_word_spam[word] = (1+spamWords[word]*1.00)/spamCount*1.0\n",
    "    p_word_ham[word] = (1+hamWords[word]*1.00)/hamCount*1.0\n",
    "\n",
    "    print word, \"\\t\", p_word_ham[word], \"\\t\", p_word_spam[word] #, \"\\t\", p_word_ham[word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x /home/cloudera/Downloads/scalingML-master/reducer341.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob188277713146083640.jar tmpDir=null\n",
      "16/11/28 09:44:38 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/11/28 09:44:38 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/11/28 09:44:39 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/11/28 09:44:39 WARN hdfs.DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1281)\n",
      "\tat java.lang.Thread.join(Thread.java:1355)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:862)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:600)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:789)\n",
      "16/11/28 09:44:39 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/11/28 09:44:40 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1480281455302_0011\n",
      "16/11/28 09:44:40 INFO impl.YarnClientImpl: Submitted application application_1480281455302_0011\n",
      "16/11/28 09:44:40 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1480281455302_0011/\n",
      "16/11/28 09:44:40 INFO mapreduce.Job: Running job: job_1480281455302_0011\n",
      "16/11/28 09:44:52 INFO mapreduce.Job: Job job_1480281455302_0011 running in uber mode : false\n",
      "16/11/28 09:44:52 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/11/28 09:45:15 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/11/28 09:45:16 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/11/28 09:45:27 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/11/28 09:45:27 INFO mapreduce.Job: Job job_1480281455302_0011 completed successfully\n",
      "16/11/28 09:45:27 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=603111\n",
      "\t\tFILE: Number of bytes written=1562516\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216481\n",
      "\t\tHDFS: Number of bytes written=252429\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=42694\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=9367\n",
      "\t\tTotal time spent by all map tasks (ms)=42694\n",
      "\t\tTotal time spent by all reduce tasks (ms)=9367\n",
      "\t\tTotal vcore-seconds taken by all map tasks=42694\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=9367\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=43718656\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=9591808\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=15594\n",
      "\t\tMap output bytes=571917\n",
      "\t\tMap output materialized bytes=603117\n",
      "\t\tInput split bytes=214\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=603117\n",
      "\t\tReduce input records=15594\n",
      "\t\tReduce output records=5490\n",
      "\t\tSpilled Records=31188\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=1064\n",
      "\t\tCPU time spent (ms)=4410\n",
      "\t\tPhysical memory (bytes) snapshot=532881408\n",
      "\t\tVirtual memory (bytes) snapshot=4511928320\n",
      "\t\tTotal committed heap usage (bytes)=391979008\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216267\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=252429\n",
      "16/11/28 09:45:27 INFO streaming.StreamJob: Output directory: nbmodelLaplace99\n"
     ]
    }
   ],
   "source": [
    "#### Model for enrollment\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar \\\n",
    "   -D mapreduce.partition.keycomparator.options=\"-k1,1\" \\\n",
    "   -mapper /home/cloudera/Downloads/scalingML-master/mapper321.py \\\n",
    "   -reducer /home/cloudera/Downloads/scalingML-master/reducer341.py \\\n",
    "   -input NB/e3.txt\\\n",
    "   -output nbmodelLaplace99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -get nbmodelLaplace99/part-00000 /home/cloudera/Downloads/scalingML-master/LaplaceSPAM_Model_MNB3.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer HAM HAM 2.4245252635e-17 1.63458429239e-18\n",
      "0001.1999-12-10.kaminski HAM HAM 1.02204684856e-11 2.82024022504e-12\n",
      "0001.2000-01-17.beck HAM HAM 1.54404228035e-76 3.42464715543e-311\n",
      "0001.2000-06-06.lokay HAM HAM 3.55816390931e-98 4.42873182711e-256\n",
      "0001.2001-02-07.kitchen HAM HAM 5.38631368375e-130 2.99390192822e-153\n",
      "0001.2001-04-02.williams HAM HAM 5.61501985991e-195 2.36439762862e-203\n",
      "0002.1999-12-13.farmer HAM SPAM 1.09343729309e-132 4.6562996096e-07\n",
      "0002.2001-02-07.kitchen HAM HAM 5.47054257876e-169 4.68931226745e-172\n",
      "0002.2001-05-25.SA_and_HP SPAM SPAM 1.1531672766e-253 1.09024749429e-225\n",
      "0002.2003-12-18.GP SPAM SPAM 6.31758887532e-217 2.50762805245e-158\n",
      "0002.2004-08-01.BG SPAM HAM 2.10818244343e-15 6.96296340852e-312\n",
      "0003.1999-12-10.kaminski HAM HAM 2.33817527134e-157 1.59453964537e-179\n",
      "0003.1999-12-14.farmer HAM HAM 1.29922929426e-30 1.12197490023e-37\n",
      "0003.2000-01-17.beck HAM HAM 7.07691707779e-156 1.12659077551e-253\n",
      "0003.2001-02-08.kitchen HAM HAM 2.41216562349e-182 7.24349917643e-273\n",
      "0003.2003-12-18.GP SPAM HAM 1.07486907317e-20 8.94819484668e-317\n",
      "0003.2004-08-01.BG SPAM SPAM 1.55957255832e-318 4.05151892351e-291\n",
      "0004.1999-12-10.kaminski HAM HAM 1.21251642766e-90 9.27615137089e-149\n",
      "0004.1999-12-14.farmer HAM HAM 3.06197348513e-96 5.69601680536e-241\n",
      "0004.2001-04-02.williams HAM HAM 2.20104656074e-259 1.35223931374e-270\n",
      "0004.2001-06-12.SA_and_HP SPAM SPAM 3.81763963402e-52 9.52402746265e-12\n",
      "0004.2004-08-01.BG SPAM SPAM 2.65807317463e-320 1.13482158509e-297\n",
      "0005.1999-12-12.kaminski HAM SPAM 3.41140671321e-296 2.11729746623e-16\n",
      "0005.1999-12-14.farmer HAM HAM 7.87695679647e-89 1.3491189421e-209\n",
      "0005.2000-06-06.lokay HAM HAM 1.68464939562e-160 1.61160476823e-166\n",
      "0005.2001-02-08.kitchen HAM HAM 2.00023334239e-15 3.55563125788e-58\n",
      "0005.2001-06-23.SA_and_HP SPAM SPAM 1.21444343975e-78 2.12817016335e-71\n",
      "0005.2003-12-18.GP SPAM HAM 7.55131298404e-244 1.40655229573e-256\n",
      "0006.1999-12-13.kaminski HAM HAM 5.33533949547e-187 1.34055201127e-194\n",
      "0006.2001-02-08.kitchen HAM SPAM 2.19771511215e-116 3.26596525836e-66\n",
      "0006.2001-04-03.williams HAM HAM 2.20872942735e-114 1.70723797092e-117\n",
      "0006.2001-06-25.SA_and_HP SPAM SPAM 2.71806684709e-155 6.36114938655e-144\n",
      "0006.2003-12-18.GP SPAM SPAM 1.18497741926e-137 1.75837981534e-102\n",
      "0006.2004-08-01.BG SPAM SPAM 2.9474182409e-105 1.11386299584e-71\n",
      "0007.1999-12-13.kaminski HAM HAM 2.68003509949e-253 1.41302774711e-321\n",
      "0007.1999-12-14.farmer HAM HAM 4.73691968039e-262 3.81339372532e-294\n",
      "Zero\n",
      "0007.2000-01-17.beck HAM HAM 4.67715035039e-80 0.0\n",
      "0007.2001-02-09.kitchen HAM SPAM 2.31661519953e-303 1.63459474298e-40\n",
      "0007.2003-12-18.GP SPAM SPAM 8.10092692815e-189 1.72430709744e-146\n",
      "0007.2004-08-01.BG SPAM HAM 2.60009168022e-13 5.26046896354e-235\n",
      "0008.2001-02-09.kitchen HAM SPAM 1.46196937442e-283 2.17891912064e-272\n",
      "0008.2001-06-12.SA_and_HP SPAM SPAM 3.81763963402e-52 9.52402746265e-12\n",
      "0008.2001-06-25.SA_and_HP SPAM SPAM 3.36251893978e-314 1.03739368233e-32\n",
      "0008.2003-12-18.GP SPAM SPAM 1.60352711777e-104 6.06914483359e-54\n",
      "0008.2004-08-01.BG SPAM HAM 7.08036141659e-107 3.92389271731e-244\n",
      "0009.1999-12-13.kaminski HAM HAM 1.05111594057e-125 7.57298124418e-221\n",
      "0009.1999-12-14.farmer HAM HAM 2.86744063608e-226 1.00547562667e-262\n",
      "0009.2000-06-07.lokay HAM HAM 4.16839746443e-49 5.64642083685e-182\n",
      "0009.2001-02-09.kitchen HAM SPAM 9.95376116384e-215 1.05663185682e-145\n",
      "0009.2001-06-26.SA_and_HP SPAM SPAM 9.21622096911e-195 7.87238255302e-154\n",
      "0009.2003-12-18.GP SPAM SPAM 0.0 1.53255986066e-293\n",
      "0010.1999-12-14.farmer HAM SPAM 5.77682478706e-193 2.23744097312e-52\n",
      "0010.1999-12-14.kaminski HAM HAM 6.6822180835e-77 4.03626503231e-86\n",
      "0010.2001-02-09.kitchen HAM SPAM 1.32510719687e-183 1.42019810398e-28\n",
      "0010.2001-06-28.SA_and_HP SPAM HAM 2.90692760133e-153 1.50470869434e-293\n",
      "0010.2003-12-18.GP SPAM SPAM 4.74206944048e-22 3.54915769156e-21\n",
      "0010.2004-08-01.BG SPAM HAM 1.08086287622e-36 1.24243522389e-238\n",
      "0011.1999-12-14.farmer HAM HAM 3.13738967759e-76 1.54747251107e-139\n",
      "0011.2001-06-28.SA_and_HP SPAM HAM 7.07739973337e-150 2.29799446179e-287\n",
      "0011.2001-06-29.SA_and_HP SPAM HAM 1.07957230151e-74 4.90730094004e-89\n",
      "0011.2003-12-18.GP SPAM SPAM 1.62582077199e-222 4.34546206109e-197\n",
      "0011.2004-08-01.BG SPAM SPAM 1.71869975609e-276 5.96888039196e-251\n",
      "0012.1999-12-14.farmer HAM SPAM 3.30892501671e-243 9.40313518954e-119\n",
      "0012.1999-12-14.kaminski HAM HAM 8.5521035379e-51 1.24784915012e-185\n",
      "0012.2000-01-17.beck HAM HAM 6.83238123186e-77 1.48219693752e-322\n",
      "0012.2000-06-08.lokay HAM HAM 2.88361004731e-21 1.43460011636e-25\n",
      "0012.2001-02-09.kitchen HAM HAM 2.80236607246e-186 3.01036323991e-198\n",
      "0012.2003-12-19.GP SPAM SPAM 7.60587632004e-67 2.28301617636e-59\n",
      "0013.1999-12-14.farmer HAM HAM 3.64944817022e-30 5.43456913045e-90\n",
      "0013.1999-12-14.kaminski HAM HAM 8.52663786531e-150 6.76223885555e-299\n",
      "0013.2001-04-03.williams HAM HAM 5.72826638779e-253 9.93080533921e-260\n",
      "0013.2001-06-30.SA_and_HP SPAM SPAM 9.2925597869e-230 3.01389377992e-180\n",
      "0013.2004-08-01.BG SPAM SPAM 5.23075696442e-283 9.83949455936e-239\n",
      "0014.1999-12-14.kaminski HAM HAM 2.77561167201e-17 3.22818235706e-178\n",
      "0014.1999-12-15.farmer HAM HAM 1.75288665672e-124 5.28051776433e-199\n",
      "0014.2001-02-12.kitchen HAM HAM 7.48712805573e-225 3.6542612296e-288\n",
      "0014.2001-07-04.SA_and_HP SPAM SPAM 1.94581891277e-217 3.51904758929e-49\n",
      "0014.2003-12-19.GP SPAM SPAM 9.54965857748e-72 1.50803252498e-65\n",
      "0014.2004-08-01.BG SPAM SPAM 0.0 1.10685492066e-303\n",
      "0015.1999-12-14.kaminski HAM HAM 3.87152007887e-258 1.21177053519e-287\n",
      "0015.1999-12-15.farmer HAM SPAM 1.13586186045e-318 2.9414038712e-57\n",
      "0015.2000-06-09.lokay HAM HAM 1.56292847546e-54 1.92626382881e-56\n",
      "0015.2001-02-12.kitchen HAM SPAM 4.209662879e-285 1.63433148301e-128\n",
      "0015.2001-07-05.SA_and_HP SPAM SPAM 1.57808833874e-66 6.81705877378e-25\n",
      "0015.2003-12-19.GP SPAM SPAM 1.25596929791e-225 1.74853101574e-160\n",
      "0016.1999-12-15.farmer HAM HAM 1.7326765209e-283 4.9193845163e-299\n",
      "0016.2001-02-12.kitchen HAM HAM 2.18290689433e-127 4.32799218123e-198\n",
      "0016.2001-07-05.SA_and_HP SPAM SPAM 1.57808833874e-66 6.81705877378e-25\n",
      "0016.2001-07-06.SA_and_HP SPAM SPAM 1.80934288061e-227 1.27949861456e-107\n",
      "0016.2003-12-19.GP SPAM HAM 3.07963291336e-11 3.2211372379e-301\n",
      "0016.2004-08-01.BG SPAM SPAM 4.54152032629e-277 1.21717478241e-245\n",
      "0017.1999-12-14.kaminski HAM HAM 1.04010324975e-144 5.8537894369e-148\n",
      "Zero\n",
      "0017.2000-01-17.beck HAM HAM 4.67715035039e-80 0.0\n",
      "0017.2001-04-03.williams HAM HAM 1.08930082829e-167 3.3920329133e-171\n",
      "0017.2003-12-18.GP SPAM SPAM 2.57986886082e-86 5.75560786453e-79\n",
      "0017.2004-08-01.BG SPAM SPAM 1.64593976373e-42 1.64174581482e-14\n",
      "0017.2004-08-02.BG SPAM SPAM 8.07080269929e-90 9.3993080554e-09\n",
      "0018.1999-12-14.kaminski HAM SPAM 4.335072782e-51 1.67016934569e-49\n",
      "0018.2001-07-13.SA_and_HP SPAM HAM 2.18176315135e-57 1.29163137467e-254\n",
      "0018.2003-12-18.GP SPAM HAM 4.35411486225e-105 2.14035553074e-314\n",
      "Accuracy:  0.76\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import re\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "f = open('/home/cloudera/Downloads/scalingML-master/LaplaceSPAM_Model_MNB3.tsv', \"r\")\n",
    "\n",
    "## use readlines to read all lines in the file\n",
    "## The variable \"lines\" is a list containing all lines\n",
    "lines = file.read(f).splitlines()\n",
    "#print lines\n",
    "\n",
    "modelSpam = {}\n",
    "modelHam = {} #defaultdict(int)\n",
    "\n",
    "for line in lines:\n",
    "        #print line\n",
    "        word, h, s = line.split('\\t')\n",
    "        word = word.strip(\" \")\n",
    "        modelHam[word] = float(h) *1.0\n",
    "        modelSpam[word] = float(s) *1.0\n",
    "f.close()\n",
    "\n",
    "#get the lines from mapper\n",
    "currentID = 0\n",
    "vocab = set() # all the words in email\n",
    "accurate = 0\n",
    "total = 0\n",
    "f = open('/home/cloudera/NaiveBayes/enronemail_1h.txt', \"r\")\n",
    "\n",
    "\n",
    "## use readlines to read all lines in the file\n",
    "## The variable \"lines\" is a list containing all lines\n",
    "lines = file.read(f).splitlines()\n",
    "#a=\"shanghai shangha 6 7\"\n",
    "\n",
    "\n",
    "#print a.split('\\t')\n",
    "for line in lines:\n",
    "    try:\n",
    "    #print line\n",
    "        iD, spam, subject, body = line.split('\\t')\n",
    "        words = body + ' ' + subject\n",
    "    except ValueError:\n",
    "        iD, spam, words = line.split('\\t')\n",
    "    #iD, spam, words = line.split('\\t')\n",
    "    p_h = 1.0\n",
    "    p_s = 1.0\n",
    "    for word in re.findall(r\"[\\w']+\", words):\n",
    "        #words.split():\n",
    "        word = word.lower()\n",
    "        #print \"words\" , word, float(modelHam[word])\n",
    "        if modelHam[word]>0.0 and p_h > 0.0:\n",
    "            if p_h is not 1.0:\n",
    "                #print p_h\n",
    "                p_h  = math.log(p_h)+math.log(float(modelHam[word]))\n",
    "               # print p_h\n",
    "                p_h = math.exp(p_h)\n",
    "                #print p_h\n",
    "            else: #\n",
    "                p_h = float(modelHam[word])\n",
    "        else:\n",
    "            p_h = float(modelHam[word])\n",
    "        \n",
    "        \n",
    "        if modelSpam[word]>0.0 and p_s > 0.0:\n",
    "            if p_s is not 1.0:\n",
    "                p_s  = math.log(p_s)+math.log(float(modelSpam[word]))\n",
    "                p_s = math.exp(p_s)\n",
    "                #print p_s\n",
    "            else:\n",
    "                p_s = float(modelSpam[word])\n",
    "        else:\n",
    "            p_s = float(modelSpam[word])\n",
    "        \n",
    "    if(int(spam)== 1):\n",
    "        c = \"SPAM\"\n",
    "    else:\n",
    "        c = \"HAM\"\n",
    "        \n",
    "    if p_s == 0.0:\n",
    "        print \"Zero\"\n",
    "\n",
    "    if p_s > p_h:\n",
    "        p_c = \"SPAM\"\n",
    "    else:\n",
    "        p_c = \"HAM\"\n",
    "    print iD, c, p_c, p_h, p_s\n",
    "\n",
    "    if(p_c == c):\n",
    "\n",
    "        accurate += 1\n",
    "    total +=1\n",
    "\n",
    "    # vocab.add(word)\n",
    "    #currentID = iD\n",
    "        \n",
    "print \"Accuracy: \", (accurate*1.0/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Accuracy increases, because the probability of a message bei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. HW2.6 Benchmark your code with the Python SciKit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import feature_extraction.text \n",
    "import csv, re, string\n",
    "import numpy as np\n",
    "\n",
    "filename = open('enron-1.txt', 'r')\n",
    "txt = filenameread().strip()\n",
    "\n",
    "emails = txt.split('\\n')\n",
    "train_label = {}\n",
    "train_data ={}\n",
    "test_label ={}\n",
    "test_data = {}\n",
    "\n",
    "for email in emails:\n",
    "    eid, tr_l, tr_data = email.strip().split('\\t') \n",
    "    train_label.add(tr_l)\n",
    "    test_label.add(tr_l)\n",
    "    #get the data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
